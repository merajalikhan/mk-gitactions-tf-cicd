name: 'Build Databricks Workflow'

on: 
  pull_request:  ##  On any pull request , trigger to initiate workflow on the given branch
     branches: [ "main" ]
  
permissions:
  contents: read

## Set Environment variables using values for Git env variables and secrets   
env: 
  TF_VAR_databricks_host: ${{ secrets.DATABRICKS_HOST }}
  TF_VAR_databricks_token: ${{ secrets.DATABRICKS_TOKEN }}
  TF_WORKSPACE: ${{vars.TF_WORKSPACE}} ## TF workspace to run the terraform code and access to shared state 

jobs:  
  build-and-test:  # Job name
    name: 'Build and Test'
    runs-on: ubuntu-latest  ## Runner
    environment:  ${{vars.GIT_ENVIRONMENT}}

    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
    defaults:
      run:
        shell: bash
        working-directory: ./Terraform

    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout Repository
      uses: actions/checkout@v3       
    
    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token  
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1
      with:
        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.
    - name: Initialize Terraform
      run: |                                       
        terraform init   

    # Generates an execution plan for Terraform
    - name: Terraform Plan
      id: plan      
      run: |          
          terraform plan -input=false

    # Check plan status    
    - name: Terraform Plan Status
      if: steps.plan.outcome == 'failure'
      run: exit 1

    # Run unit tests - use databrick's  run-notebook action to run a notebook. The notebook contains unit tests code
    - name: Run Unit Tests    
            
      uses: databricks/run-notebook@v0
      with:
          local-notebook-path: Terraform/Notebooks/MK-TEST-20230518.py
          databricks-host: ${{ secrets.DATABRICKS_HOST }}
          databricks-token: ${{ secrets.DATABRICKS_TOKEN }}
          git-commit: ${{ github.event.pull_request.head.sha }}
          new-cluster-json: >
            {
              "num_workers": 1,
              "spark_version": "12.2.x-scala2.12", 
              "node_type_id": "Standard_DS3_v2"
            }  
          access-control-list-json: >
            [
              {
                "group_name": "users",
                "permission_level": "CAN_VIEW"
              }
            ] 

    # Archive artifacats - (ex : Terraform config files / unit test results etc)    
    - name: Archive artifacts
      uses: actions/upload-artifact@v3
      with:
        name: tf-files
        path: |
          Terraform
          !.terraform
          retention-days: 2
      




       




    
         
      
      
      
      
